# -*- coding: utf-8 -*-
"""Datamining-Books.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DcceQkU0KXdZT4IzfwdIt_470yDEUaxp

# **Installing & Calling Libraries**
"""

!pip install isbnlib
!pip install newspaper3k
!pip install goodreads_api_client
!pip install scikit-learn-extra

import numpy as np 
import pandas as pd
import os
import seaborn as sns
import isbnlib
from newspaper import Article
import matplotlib.pyplot as plt
plt.style.use('ggplot')
from tqdm import tqdm
from progressbar import ProgressBar
import re
from scipy.cluster.vq import kmeans, vq
from pylab import plot, show
from matplotlib.lines import Line2D
import matplotlib.colors as mcolors
import goodreads_api_client as gr
from sklearn.cluster import KMeans
from sklearn import neighbors
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import warnings
warnings.filterwarnings("ignore")
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler

"""# **Uploading & Reading Dataset From Local Drive**"""

from google.colab import files
data = files.upload()

df = pd.read_csv('books.csv', error_bad_lines = False)

"""# **EDA**"""

df.index = df['bookID']
print("Dataset contains {} rows and {} columns".format(df.shape[0], df.shape[1]))

df.replace(to_replace='J.K. Rowling-Mary GrandPré', value = 'J.K. Rowling', inplace=True)

df.head()

df.tail()

df.dtypes

"""**books with most occurances in the list**"""

sns.set_context('poster')
plt.figure(figsize=(20,15))
books = df['title'].value_counts()[:20]
rating = df.average_rating[:20]
sns.barplot(x = books, y = books.index, palette='deep')
plt.title("Most Occurring Books")
plt.xlabel("Number of occurances")
plt.ylabel("Books")
plt.show()

"""**distribution of books for all languages**"""

sns.set_context('paper')
plt.figure(figsize=(15,10))
ax = df.groupby('language_code')['title'].count().plot.bar()
plt.title('Language Code')
plt.xticks(fontsize = 15)
for p in ax.patches:
    ax.annotate(str(p.get_height()), (p.get_x()-0.3, p.get_height()+100))

"""**top 10 most rated books**"""

most_rated = df.sort_values('ratings_count', ascending = False).head(10).set_index('title')
plt.figure(figsize=(15,10))
sns.barplot(most_rated['ratings_count'], most_rated.index, palette='rocket')

"""**the authors with most books**"""

sns.set_context('talk')
most_books = df.groupby('authors')['title'].count().reset_index().sort_values('title', ascending=False).head(10).set_index('authors')
plt.figure(figsize=(15,10))
ax = sns.barplot(most_books['title'], most_books.index, palette='icefire_r')
ax.set_title("Top 10 authors with most books")
ax.set_xlabel("Total number of books")
for i in ax.patches:
    ax.text(i.get_width()+.3, i.get_y()+0.5, str(round(i.get_width())), fontsize = 10, color = 'k')

"""**top 10 highly rated authors**"""

high_rated_author = df[df['average_rating']>=4.3]
high_rated_author = high_rated_author.groupby('authors')['title'].count().reset_index().sort_values('title', ascending = False).head(10).set_index('authors')
plt.figure(figsize=(15,10))
ax = sns.barplot(high_rated_author['title'], high_rated_author.index, palette='Set2')
ax.set_xlabel("Number of Books")
ax.set_ylabel("Authors")
for i in ax.patches:
    ax.text(i.get_width()+.3, i.get_y()+0.5, str(round(i.get_width())), fontsize = 10, color = 'k')

"""**rating distribution for the books**"""

def segregation(data):
    values = []
    for val in data.average_rating:
        if val>=0 and val<=1:
            values.append("Between 0 and 1")
        elif val>1 and val<=2:
            values.append("Between 1 and 2")
        elif val>2 and val<=3:
            values.append("Between 2 and 3")
        elif val>3 and val<=4:
            values.append("Between 3 and 4")
        elif val>4 and val<=5:
            values.append("Between 4 and 5")
        else:
            values.append("NaN")
    print(len(values))
    return values

plt.figure(figsize=(10,10))
rating= df.average_rating.astype(float)
sns.distplot(rating, bins=20)

df1 = pd.read_csv('books.csv', error_bad_lines = False)
df1.replace(to_replace='J.K. Rowling-Mary GrandPré', value = 'J.K. Rowling', inplace=True)
df1['Ratings_Dist'] = segregation(df1)
ratings_pie = df1['Ratings_Dist'].value_counts().reset_index()
labels = ratings_pie['index']
colors = ['lightblue','darkmagenta','coral','bisque', 'black']
percent = 100.*ratings_pie['Ratings_Dist']/ratings_pie['Ratings_Dist'].sum()
fig, ax1 = plt.subplots()
ax1.pie(ratings_pie['Ratings_Dist'],colors = colors, 
        pctdistance=0.85, startangle=90, explode=(0.05, 0.05, 0.05, 0.05, 0.05))
#Draw a circle now:
centre_circle = plt.Circle((0,0), 0.70, fc ='white')
fig1 = plt.gcf()
fig1.gca().add_artist(centre_circle)
#Equal Aspect ratio ensures that pie is drawn as a circle
plt.axis('equal')
plt.tight_layout()
labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(labels, percent)]
plt.legend( labels, loc = 'best',bbox_to_anchor=(-0.1, 1.),)

"""**relationship between ratings and review counts**"""

plt.figure(figsize=(15,10))
df.dropna(0, inplace=True)
sns.set_context('paper')
ax =sns.jointplot(x="average_rating",y='text_reviews_count', kind='scatter',  data= df[['text_reviews_count', 'average_rating']])
ax.set_axis_labels("Average Rating", "Text Review Count")
plt.show()

trial = df[~(df['text_reviews_count']>5000)]

plt.figure(figsize=(15,10))
df.dropna(0, inplace=True)
sns.set_context('paper')
ax =sns.jointplot(x="average_rating",y='text_reviews_count', kind='scatter',  data= trial, color = 'green')
ax.set_axis_labels("Average Rating", "Text Review Count")
plt.show()

"""**relationship between number of pages and ratings**"""

plt.figure(figsize=(15,10))
sns.set_context('paper')
ax = sns.jointplot(x="average_rating", y="# num_pages", data = df, color = 'crimson')
ax.set_axis_labels("Average Rating", "Number of Pages")

trial = df[~(df['# num_pages']>1000)]

ax = sns.jointplot(x="average_rating", y="# num_pages", data = trial, color = 'darkcyan')
ax.set_axis_labels("Average Rating", "Number of Pages")

"""**relationship between ratings and ratings count**"""

sns.set_context('paper')
ax = sns.jointplot(x="average_rating", y="ratings_count", data = df, color = 'blueviolet')
ax.set_axis_labels("Average Rating", "Ratings Count")

trial = df[~(df.ratings_count>2000000)]

sns.set_context('paper')
ax = sns.jointplot(x="average_rating", y="ratings_count", data = trial, color = 'brown')
ax.set_axis_labels("Average Rating", "Ratings Count")

"""**books with the highest reviews**"""

most_text = df.sort_values('text_reviews_count', ascending = False).head(10).set_index('title')
plt.figure(figsize=(15,10))
sns.set_context('poster')
ax = sns.barplot(most_text['text_reviews_count'], most_text.index, palette='magma')
for i in ax.patches:
    ax.text(i.get_width()+2, i.get_y()+0.5,str(round(i.get_width())), fontsize=10,color='black')
plt.show()

"""# **Data Pre-Processing**

**Basic inspections & Removing Useless Columns**
"""

columns_to_drop = ['bookID', 'isbn', 'isbn13']
df.drop(columns=columns_to_drop, inplace=True)
df

"""**Nulls & Duplicates**"""

df.isnull().sum()

df[df.duplicated()]

"""**Removing Outliers**"""

from collections import Counter

def detect_outliers(data_frame=df, n=2):
  
    
    numeric_columns = data_frame.select_dtypes(include=[np.number])
    outlires_indicies = []
    
    for column in numeric_columns:
        
        #  first quartile - 25 %
        Q1 = np.nanpercentile(data_frame[column], 25)
        
        #  second quartile - 75%
        Q2 = np.nanpercentile(data_frame[column], 75)
        
        #  calculating interquartile range
        IQR = Q2 - Q1
        
        # setting the bounds
        lower_bound = Q1 - (1.5 * IQR)
        upper_bound = Q2 + (1.5 * IQR)
        
        # indexes of the outliers
        outliers = data_frame[(data_frame[column] < lower_bound) | (data_frame[column] > upper_bound)].index
        
        #  extending the indicies to the list
        outlires_indicies.extend(outliers)
    
    #  this frequency table is useful to see occurences of outliers in rows
    freq_table = Counter(outlires_indicies)
    
    #  showing only indicies where theres more than n outliers
    return [index for index, occurences in freq_table.items() if occurences > n]

outliers = detect_outliers()
df.drop(index=outliers, inplace=True)
f'outliers droped - {len(outliers)}'

df.shape

"""**Data Standardization**"""

from sklearn.preprocessing import scale
cols = ['ratings_count', 'text_reviews_count']
df[cols] = scale(df[cols])
df

"""# **Train & Test Split**"""

df_train,df_test=train_test_split(df,test_size=0.2)

df_train.shape

df_test.shape

"""**Train & Test Configuration for Clustering**"""

trial_train = df_train[['average_rating', 'ratings_count']]
data_train = np.asarray([np.asarray(trial_train['average_rating']), np.asarray(trial_train['ratings_count'])]).T

trial_test = df_test[['average_rating', 'ratings_count']]
data_test = np.asarray([np.asarray(trial_test['average_rating']), np.asarray(trial_test['ratings_count'])]).T

"""# **Clustering Modeling**

**K-Means.Train**

Identifying Number of Clusters
"""

X = data_train
distortions = []
for k in range(2,30):
    k_means = KMeans(n_clusters = k)
    k_means.fit(X)
    distortions.append(k_means.inertia_)

fig = plt.figure(figsize=(15,10))
plt.plot(range(2,30), distortions, 'bx-')
plt.title("Elbow Curve")

from numpy import unique
from numpy import where
from sklearn.cluster import KMeans
from matplotlib import pyplot
X = data_train
# define the model
model = KMeans(n_clusters=5)
# fit the model
model.fit(X)
# assign a cluster to each example
yhat = model.predict(X)
# retrieve unique clusters
clusters = unique(yhat)
# create scatter plot for samples from each cluster
for cluster in clusters:
	# get row indexes for samples with this cluster
	row_ix = where(yhat == cluster)
	# create scatter of these samples
	pyplot.scatter(X[row_ix, 0], X[row_ix, 1])
# show the plot
pyplot.show()

"""**K-Means.Test**"""

X_test = data_test
distortions = []
for k in range(2,30):
    k_means = KMeans(n_clusters = k)
    k_means.fit(X_test)
    distortions.append(k_means.inertia_)

fig = plt.figure(figsize=(15,10))
plt.plot(range(2,30), distortions, 'bx-')
plt.title("Elbow Curve")

X = data_test
# define the model
model = KMeans(n_clusters=5)
# fit the model
model.fit(X)
# assign a cluster to each example
yhat = model.predict(X)
# retrieve unique clusters
clusters = unique(yhat)
# create scatter plot for samples from each cluster
for cluster in clusters:
	# get row indexes for samples with this cluster
	row_ix = where(yhat == cluster)
	# create scatter of these samples
	pyplot.scatter(X[row_ix, 0], X[row_ix, 1])
# show the plot
pyplot.show()

"""**Agglomerative.Train**"""

from sklearn.cluster import AgglomerativeClustering
X = data_train
# define the model
model = AgglomerativeClustering(n_clusters=5)
# fit model and predict clusters
yhat = model.fit_predict(X)
# retrieve unique clusters
clusters = unique(yhat)
# create scatter plot for samples from each cluster
for cluster in clusters:
	# get row indexes for samples with this cluster
	row_ix = where(yhat == cluster)
	# create scatter of these samples
	pyplot.scatter(X[row_ix, 0], X[row_ix, 1])
# show the plot
pyplot.show()

"""**Agglomerative.Test**"""

X = data_test
# define the model
model = AgglomerativeClustering(n_clusters=5)
# fit model and predict clusters
yhat = model.fit_predict(X)
# retrieve unique clusters
clusters = unique(yhat)
# create scatter plot for samples from each cluster
for cluster in clusters:
	# get row indexes for samples with this cluster
	row_ix = where(yhat == cluster)
	# create scatter of these samples
	pyplot.scatter(X[row_ix, 0], X[row_ix, 1])
# show the plot
pyplot.show()

"""**BIRCH.Train**"""

from sklearn.cluster import Birch
X = data_train
# define the model
model = Birch(threshold=0.01, n_clusters=5)
# fit the model
model.fit(X)
# assign a cluster to each example
yhat = model.predict(X)
# retrieve unique clusters
clusters = unique(yhat)
# create scatter plot for samples from each cluster
for cluster in clusters:
	# get row indexes for samples with this cluster
	row_ix = where(yhat == cluster)
	# create scatter of these samples
	pyplot.scatter(X[row_ix, 0], X[row_ix, 1])
# show the plot
pyplot.show()

"""**BIRCH.Test**"""

X = data_test
# define the model
model = Birch(threshold=0.01, n_clusters=5)
# fit the model
model.fit(X)
# assign a cluster to each example
yhat = model.predict(X)
# retrieve unique clusters
clusters = unique(yhat)
# create scatter plot for samples from each cluster
for cluster in clusters:
	# get row indexes for samples with this cluster
	row_ix = where(yhat == cluster)
	# create scatter of these samples
	pyplot.scatter(X[row_ix, 0], X[row_ix, 1])
# show the plot
pyplot.show()

"""**Gaussian Mixture.Train**"""

# gaussian mixture clustering
from sklearn.mixture import GaussianMixture
X = data_train
# define the model
model = GaussianMixture(n_components=5)
# fit the model
model.fit(X)
# assign a cluster to each example
yhat = model.predict(X)
# retrieve unique clusters
clusters = unique(yhat)
# create scatter plot for samples from each cluster
for cluster in clusters:
	# get row indexes for samples with this cluster
	row_ix = where(yhat == cluster)
	# create scatter of these samples
	pyplot.scatter(X[row_ix, 0], X[row_ix, 1])
# show the plot
pyplot.show()

"""**Gaussian Mixture.Test**"""

X = data_test
# define the model
model = GaussianMixture(n_components=5)
# fit the model
model.fit(X)
# assign a cluster to each example
yhat = model.predict(X)
# retrieve unique clusters
clusters = unique(yhat)
# create scatter plot for samples from each cluster
for cluster in clusters:
	# get row indexes for samples with this cluster
	row_ix = where(yhat == cluster)
	# create scatter of these samples
	pyplot.scatter(X[row_ix, 0], X[row_ix, 1])
# show the plot
pyplot.show()

"""# **Recommendation Engine**"""

df['Ratings_Dist'] = segregation(df)

books_features = pd.concat([df['Ratings_Dist'].str.get_dummies(sep=","), df['average_rating'], df['ratings_count']], axis=1)

books_features.head()

min_max_scaler = MinMaxScaler()
books_features = min_max_scaler.fit_transform(books_features)

np.round(books_features, 2)

model = neighbors.NearestNeighbors(n_neighbors=6, algorithm='ball_tree')
model.fit(books_features)
distance, indices = model.kneighbors(books_features)

def get_index_from_name(name):
    return df[df["title"]==name].index.tolist()[0]

all_books_names = list(df.title.values)

def get_id_from_partial_name(partial):
    for name in all_books_names:
        if partial in name:
            print(name,all_books_names.index(name))
            
def print_similar_books(query=None,id=None):
    if id:
        for id in indices[id][1:]:
            print(df.iloc[id]["title"])
    if query:
        found_id = get_index_from_name(query)
        for id in indices[found_id][1:]:
            print(df.iloc[id]["title"])

"""**Examples for Engine**"""

print_similar_books("The Catcher in the Rye")

print_similar_books("The Hobbit or There and Back Again")

get_id_from_partial_name("Harry Potter and the ")

print_similar_books(id = 1) #ID for the Book 5